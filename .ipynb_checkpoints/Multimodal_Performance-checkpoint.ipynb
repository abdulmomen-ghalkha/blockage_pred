{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d98c0d0e-f3ab-4f73-8dff-e34daeaf8004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c6f2269-f63c-4de1-a488-67d6d5b1d99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../Carla/datasets/scenario25_dev/unit1/mmWave_data\\\\mmWave_power_79868.txt', '../Carla/datasets/scenario25_dev/unit1/mmWave_data\\\\mmWave_power_79868_fixed.txt']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# Define the pattern to match files\n",
    "path = \"../Carla/datasets/scenario25_dev/unit1/mmWave_data/mmWave_power_79868*\"\n",
    "files = glob.glob(path)\n",
    "print(files)\n",
    "print(files[0][0:-4] + \"_fixed.txt\" == files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e4e064-c9ef-43fc-9367-122291c1bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FutureClearWindowDataset(Dataset):\n",
    "    def __init__(self, csv_path, root_dir='.', window_length=16, T_f=5, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.root_dir = root_dir\n",
    "        self.window_length = window_length\n",
    "        self.T_f = T_f\n",
    "        self.transform = transform\n",
    "        self.resize = transforms.Resize((224, 224))  # Resize to target dimensions\n",
    "\n",
    "\n",
    "        # Total number of valid sliding windows\n",
    "        self.valid_indices = [\n",
    "            i for i in range(len(self.df) - window_length - T_f)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.valid_indices[idx]\n",
    "        end_idx = start_idx + self.window_length\n",
    "        future_start = end_idx\n",
    "        future_end = future_start + self.T_f\n",
    "\n",
    "        window_df = self.df.iloc[start_idx:end_idx]\n",
    "        future_df = self.df.iloc[future_start:future_end]\n",
    "\n",
    "        # Load data for window\n",
    "        lidar_frames = []\n",
    "        power_frames = []\n",
    "        rgb_frames = []\n",
    "\n",
    "        for row in window_df.itertuples():\n",
    "            lidar = loadmat(os.path.join(self.root_dir, row.unit1_lidar_SCR))['data']\n",
    "            power = np.loadtxt(os.path.join(self.root_dir, row.unit1_pwr_60ghz)[0:-4] + \"_fixed.txt\")\n",
    "            rgb = Image.open(os.path.join(self.root_dir, row.unit1_rgb)).convert('L')\n",
    "            rgb = self.resize(rgb)\n",
    "            lidar_frames.append(lidar)\n",
    "            power_frames.append(power)\n",
    "            rgb_frames.append(rgb)\n",
    "        lidar = torch.tensor(np.stack(lidar_frames), dtype=torch.float32).permute(2, 0, 1)\n",
    "        lidar[0, :, :] = lidar[0, :, :] / 16.392\n",
    "        lidar[1, :, :] = (lidar[1] - (-2.0941)) / (1.5621 - (-2.0941))\n",
    "        power = torch.tensor(np.stack(power_frames), dtype=torch.float32)\n",
    "        rgb = torch.tensor(np.stack(rgb_frames), dtype=torch.float32)\n",
    "\n",
    "        # Future label logic: label = 1 if no blockage in future window\n",
    "        future_blockages = future_df['blockage_label'].astype(float).values\n",
    "        label = 1 - int(np.all(future_blockages == 0))  # 1 = clear, 0 = blockage ahead\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'lidar': lidar,      # shape (window_length, ...)\n",
    "            'power': power,\n",
    "            'rgb': rgb,\n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58540aa-6bb9-48de-ba54-5192fce44353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = FutureClearWindowDataset(\n",
    "    csv_path='Deepsense_dataset.csv',\n",
    "    window_length=16,\n",
    "    T_f=5\n",
    ")\n",
    "\n",
    "# Load from file\n",
    "sample_weights_loaded = torch.load('sample_weights.pt', weights_only=True)\n",
    "\n",
    "\n",
    "# Create sampler\n",
    "sampler = WeightedRandomSampler(sample_weights_loaded, num_samples=len(sample_weights_loaded), replacement=True)\n",
    "\n",
    "# Create DataLoader\n",
    "loader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch['lidar'].shape, batch['power'].shape, batch['rgb'].shape, batch['label'])\n",
    "    print(torch.max(batch['power']))\n",
    "    print(batch['power'])\n",
    "    \n",
    "    max_value, max_index = torch.max(batch['power'], dim=1)\n",
    "    \n",
    "    batch_number = max_index // batch['power'].size(1)\n",
    "    location_within_batch = max_index % batch['power'].size(1)\n",
    "    \n",
    "    print(f\"The maximum value is {max_value} at batch number {batch_number} and location {location_within_batch}\")\n",
    "    print(batch['power'])\n",
    "\n",
    "\n",
    "    #print(torch.max(batch['lidar'][:, 0, :, :]), torch.max(batch['lidar'][:, 1, :, :]), torch.min(batch['lidar'][:, 1, :, :]))\n",
    "    rgb_frames = batch['rgb']  # shape: (batch_size, num_frames, channels, height, width)\n",
    "\n",
    "    # Let's plot the first image from the first batch (index 0)\n",
    "    # We'll visualize a few frames from the first example in the batch\n",
    "\n",
    "    num_frames_to_plot = 15  # Number of frames to visualize\n",
    "    fig, axes = plt.subplots(1, num_frames_to_plot, figsize=(15, 5))\n",
    "    print(batch[\"label\"][0])\n",
    "    for i in range(num_frames_to_plot):\n",
    "        frame = rgb_frames[0, i, :, :]  # Get the first frame, with shape (height, width)\n",
    "        frame = frame.numpy()  # Convert tensor to numpy array for plotting\n",
    "        axes[i].imshow(frame, cmap='gray')  # Display the image in grayscale\n",
    "        axes[i].axis('off')  # Turn off axis labels\n",
    "    \n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e21cd6-930c-4c2b-aaa1-0f7933352ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SymmetricLiDARCNN(nn.Module):\n",
    "    def __init__(self, input_channels=2, num_classes=2):\n",
    "        super(SymmetricLiDARCNN, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(32),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))  # Makes it input-size agnostic\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c98118b-0b00-44f5-8cf1-01b74a5768fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BlockagePredictionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2, dropout_rate=0.01):\n",
    "        super(BlockagePredictionCNN, self).__init__()\n",
    "\n",
    "        # 3D convolution to process both temporal and spatial data\n",
    "        self.conv1 = nn.Conv3d(16, 16, kernel_size=(3, 7, 7), padding=(1, 3, 3))\n",
    "        self.bn1 = nn.BatchNorm3d(16)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv3d(16, 32, kernel_size=(3, 5, 5), padding=(1, 2, 2))\n",
    "        self.bn2 = nn.BatchNorm3d(32)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "\n",
    "        self.conv3 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn3 = nn.BatchNorm3d(64)\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "\n",
    "        # Fusion stack\n",
    "        self.fusion_conv1 = nn.Conv3d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn_fusion1 = nn.BatchNorm3d(128)\n",
    "        self.fusion_pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "\n",
    "        # Dropout and fully connected layers\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(128 * 4 * 7 * 7, num_classes)  # Adjusted based on the output size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 16, 224, 224)\n",
    "\n",
    "        x = x.unsqueeze(2)  # (B, 16, 1, 224, 224)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.fusion_conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn_fusion1(x)\n",
    "        x = self.fusion_pool1(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)  # Flatten except batch dimension\n",
    "        # x = self.dropout(x)  # you commented it out in your version\n",
    "\n",
    "        print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd8bb5b-0cc3-4cce-b93c-c3e342977b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class mmWaveSCRNet(nn.Module):\n",
    "    def __init__(self, input_channels=16, num_classes=2, dropout_rate=0.2):\n",
    "        super(mmWaveSCRNet, self).__init__()\n",
    "\n",
    "        # Stack 1\n",
    "        self.conv1 = nn.Conv2d(input_channels, 4, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.bn1 = nn.BatchNorm2d(4)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(1, 2))  # Reduce W: 64 → 32\n",
    "\n",
    "        # Stack 2\n",
    "        self.conv2 = nn.Conv2d(4, 8, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(8)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(1, 2))  # Reduce W: 32 → 16\n",
    "\n",
    "        # Fusion Stack\n",
    "        self.fusion_conv1 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn_fusion1 = nn.BatchNorm2d(16)\n",
    "        self.fusion_pool1 = nn.MaxPool2d(kernel_size=(1, 2))  # W: 16 → 8\n",
    "\n",
    "        self.fusion_conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.bn_fusion2 = nn.BatchNorm2d(16)\n",
    "        self.fusion_pool2 = nn.MaxPool2d(kernel_size=(1, 2))  # W: 8 → 4\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(16 * 1 * 4, num_classes)  # Final shape (B, 16, 1, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)  # (B, 16, 1, 64)\n",
    "\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # (B, 4, 1, 64)\n",
    "        x = self.pool1(x)                    # (B, 4, 1, 32)\n",
    "\n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # (B, 8, 1, 32)\n",
    "        x = self.pool2(x)                    # (B, 8, 1, 16)\n",
    "\n",
    "        x = F.relu(self.bn_fusion1(self.fusion_conv1(x)))  # (B, 16, 1, 16)\n",
    "        x = self.fusion_pool1(x)                           # (B, 16, 1, 8)\n",
    "\n",
    "        x = F.relu(self.bn_fusion2(self.fusion_conv2(x)))  # (B, 16, 1, 8)\n",
    "        x = self.fusion_pool2(x)                           # (B, 16, 1, 4)\n",
    "\n",
    "        x = torch.flatten(x, 1)                            # (B, 16*1*4)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c720d2-9479-4190-ae47-5112d5fc4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_lidar = mmWaveSCRNet(input_channels=16, num_classes=2).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "class_weights = {0: 1.0932087765216907, 1: 6.728603435399553}\n",
    "weights_tensor = torch.tensor([class_weights[i] for i in range(len(class_weights))], dtype=torch.float).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_lidar.parameters(), lr=1e-4)\n",
    "\n",
    "# Lists to store loss and accuracy\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "\n",
    "# Training\n",
    "model_lidar.train()\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for _, inputs in zip(range(10), loader):\n",
    "        labels = inputs[\"label\"].to(device)\n",
    "        inputs = inputs[\"power\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_lidar(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        print(labels, predicted)\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    loss_history.append(epoch_loss)\n",
    "    acc_history.append(epoch_acc)\n",
    "    print(epoch_acc)\n",
    "    \n",
    "# Plotting\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(loss_history, marker='o')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(acc_history, marker='o')\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7613f87-dc81-49f0-afea-e07bc50731d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
